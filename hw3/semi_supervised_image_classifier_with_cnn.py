#!/usr/bin/python3
# coding: utf-8

import numpy as np

# using structure of ImageNet as reference (Ref: https://arxiv.org/abs/1512.03385)
def CNN(training_X, training_Y, testing_X, testing_Y, nb_epoch=1000, output_model_path='cifar10_cnn_keras_model.hdf5'):
    from keras.models import Sequential, load_model
    from keras.layers import Dense, Dropout, Activation, Flatten
    from keras.layers import PReLU
    from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D
    from keras.regularizers import l2, activity_l2
    from keras.preprocessing.image import ImageDataGenerator
    from keras.optimizers import Adam
    from keras.callbacks import ModelCheckpoint, EarlyStopping
    from keras.utils import np_utils

    batch_size = int(len(training_X)*0.01)
    nb_classes = 10

    # input image dimensions
    # the CIFAR10 images are RGB
    img_channels, img_rows, img_cols = training_X.shape[1:]
    
    training_X = training_X.astype('float32') / 255
    testing_X = testing_X.astype('float32') / 255
    training_Y = np_utils.to_categorical(training_Y, nb_classes)
    testing_Y = np_utils.to_categorical(testing_Y, nb_classes)

    print("Training data shape: {}".format(training_X.shape))
    print('# of train samples: {}'.format(len(training_Y)))
    print('# of test samples: {}'.format(len(testing_Y)))
    
    # -----initial layers-----
    model = 0 # ensure model is clear
    model = Sequential()
    model.add(Convolution2D(32, 7, 7, border_mode='same', subsample=(2, 2), input_shape=(img_channels, img_rows, img_cols),
                            dim_ordering='th', activity_regularizer=activity_l2(0.01)))
    model.add(PReLU(init='zero', weights=None))
    model.add(Dropout(0.25))
    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))
    
    model.add(Convolution2D(64, 3, 3))
    model.add(PReLU())
    model.add(ZeroPadding2D(padding=(1, 1), dim_ordering='default'))
    model.add(Convolution2D(128, 3, 3))
    model.add(PReLU())
    model.add(Dropout(0.25))
    
    model.add(ZeroPadding2D(padding=(1, 1)))
    
    model.add(AveragePooling2D(pool_size=(3, 3), strides=(2, 2)))
    model.add(Dropout(0.25))
    
    model.add(Flatten())    
    model.add(Dense(64))
    model.add(PReLU())
    model.add(Dropout(0.25))
    model.add(Dense(32))
    model.add(PReLU())
    model.add(Dropout(0.25))
    model.add(Dense(32, activation='sigmoid'))
    model.add(Dropout(0.25))
    model.add(Dense(nb_classes, activation='softmax'))
    # -----start-----
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    # print(model.summary())
    checkpointer = ModelCheckpoint(filepath=output_model_path, monitor='val_loss', verbose=1, save_best_only=True)
    earlystopping = EarlyStopping(monitor='val_loss', patience=int(nb_epoch*0.2), verbose=1)

    # this will do preprocessing and realtime data augmentation
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

    # compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied)
    datagen.fit(training_X)

    # fit the model on the batches generated by datagen.flow()
    model.fit_generator(datagen.flow(training_X, training_Y, batch_size=batch_size), 
                        samples_per_epoch=training_X.shape[0], nb_epoch=nb_epoch, verbose=2, 
                        callbacks=[checkpointer, earlystopping], 
                        validation_data=(testing_X, testing_Y))

def initial_training(training_x, training_y, testing_x, testing_y, unlabel_data, model_path, self_train=True):
    import scipy
    from keras.models import load_model    
    
    rnd = 4 if self_train else 2
    
    # ===== self training =====
    for turn in range(1, rnd):
        # -----(Re)train with CNN-----
        CNN(training_x, training_y, testing_x, testing_y, 1500*turn)
        
        # =====unlabel prediction after initial model trained=====
        model = load_model(model_path)
        print("---Making Predictions---")
        predict_probability = model.predict_proba(unlabel_data, batch_size=32, verbose=1)
        predict_class = np.argmax(predict_probability, axis=1)
        predict_entropy = scipy.stats.entropy(predict_probability.T)
        
        # -----extract n most ensure prediction from unlabel data-----
        order = np.argsort(predict_entropy, axis=0)
        predict_entropy = predict_entropy[order]
        predict_class = predict_class[order]
        unlabel_data = unlabel_data[order]

        pseudo_label_data_list = []

        for idx, i in enumerate(predict_entropy):
            if i <= predict_entropy.mean() and (len(pseudo_label_data_list) < len(predict_class)*0.25):
                pseudo_label_data_list.append(idx)
        pseudo_unlabel_x = unlabel_data[pseudo_label_data_list]
        pseudo_unlabel_y = predict_class[pseudo_label_data_list]
        
        unlabel_data = np.delete(unlabel_data, pseudo_label_data_list, axis=0) # remove from unlabel dataset
        
        # -----add dataset into training dataset-----
        training_x = np.vstack((training_x, pseudo_unlabel_x))
        training_y = np.vstack((training_y, pseudo_unlabel_y.reshape(-1, 1)))

if __name__ == "__main__":
    from sys import argv
    from os import getcwd
    
    import data_import
    
    # directory_path = getcwd()
    # model_path = "cifar10_cnn_keras_model.hdf5"
    
    if (len(argv) < 2):
        directory_path = getcwd()
        model_path = "cifar10_cnn_keras_model.hdf5"
    else:
        directory_path, model_path = argv[1], argv[2]
        
    # load data
    print("Loading Data...", end='')
    training_x, training_y, testing_x, testing_y, unlabel_data = data_import.load_data(case=1, test_ratio=0.1, directory_path=directory_path)
    print("Completed")
    
    initial_training(training_x, training_y, testing_x, testing_y, unlabel_data, model_path)
    print("=====Train Completed=====")

